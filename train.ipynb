{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 21:27:34.186582: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-05 21:27:34.189152: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-05 21:27:34.197051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-05 21:27:34.211574: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-05 21:27:34.211604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-05 21:27:34.222208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 21:27:34.756058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-05 21:27:35.434926: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-05 21:27:35.435361: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from model.destr_model import ObjDetSplitTransformer, train_one_step, validate\n",
    "from utils.data_loader import load_data_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cls = 8\n",
    "EPOCH_NUMS = 20\n",
    "BATCH_SIZE = 8\n",
    "checkpoint_dir = '/workspace/models/checkpoints'\n",
    "\n",
    "train_size = 5000\n",
    "valid_size = 500\n",
    "\n",
    "load_from_ckpt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/cv_project/cv_project/lib/python3.12/site-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'obj_det_split_transformer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "destr_block = ObjDetSplitTransformer(input_shape=(224, 224, 3), num_cls=num_cls)\n",
    "\n",
    "img = tf.keras.Input(shape=(224, 224, 3), dtype=tf.float32)\n",
    "cls_output, reg_output, total_proposals = destr_block(img)\n",
    "\n",
    "model = tf.keras.Model(inputs=img, outputs=[cls_output, reg_output, total_proposals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from latest checkpoint\n",
    "\n",
    "if load_from_ckpt:\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = '/media/daniel/DatasetIMDB/imdb_chunks'\n",
    "container_dir = '/workspace/data/tfrecords'\n",
    "\n",
    "full_dataset = load_data_tfrecord(path_to_tfrecord=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: \n",
      "\n",
      "          \t train_loss: 1.1565 1.6737,\n",
      "          \t valid loss: 1.0775 1.6343\n",
      "epoch 12: \n",
      "\n",
      "          \t train_loss: 1.0712 1.5256,\n",
      "          \t valid loss: 1.0024 1.4128\n",
      "epoch 13: \n",
      "\n",
      "          \t train_loss: 0.9998 1.3594,\n",
      "          \t valid loss: 0.9371 1.3440\n",
      "epoch 14: \n",
      "\n",
      "          \t train_loss: 0.9215 1.3175,\n",
      "          \t valid loss: 0.8864 1.3470\n",
      "epoch 15: \n",
      "\n",
      "          \t train_loss: 0.8883 1.3861,\n",
      "          \t valid loss: 0.8180 1.3234\n",
      "epoch 16: \n",
      "\n",
      "          \t train_loss: 0.8222 1.3438,\n",
      "          \t valid loss: 0.8059 1.2797\n",
      "epoch 17: \n",
      "\n",
      "          \t train_loss: 0.7994 1.2916,\n",
      "          \t valid loss: 0.7466 1.2310\n",
      "epoch 18: \n",
      "\n",
      "          \t train_loss: 0.7840 1.1898,\n",
      "          \t valid loss: 0.7262 1.2290\n",
      "epoch 19: \n",
      "\n",
      "          \t train_loss: 0.7264 1.1851,\n",
      "          \t valid loss: 0.7638 1.1876\n",
      "epoch 20: \n",
      "\n",
      "          \t train_loss: 0.7203 1.2228,\n",
      "          \t valid loss: 0.7063 1.1493\n"
     ]
    }
   ],
   "source": [
    "loss_history = {'train_loss': [], 'valid_loss': []}\n",
    "optimizers = {'mini_det': tf.keras.optimizers.Adam(learning_rate=0.00001), 'destr': tf.keras.optimizers.Adam(learning_rate=0.00001)}\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001)\n",
    "\n",
    "for epoch_idx in range(10, EPOCH_NUMS+10):\n",
    "    dataset = full_dataset.shuffle(buffer_size=5000).batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    epoch_dataset = dataset.take(count=train_size+valid_size)\n",
    "    train_dataset = epoch_dataset.take(count=train_size)\n",
    "    valid_dataset = epoch_dataset.skip(count=train_size)\n",
    "\n",
    "    total_md_loss, total_loss, step = 0, 0, 0\n",
    "    for batch in train_dataset:\n",
    "        logits, coord, label, oh_label = batch\n",
    "        \n",
    "        mini_det_loss, model_loss = train_one_step(\n",
    "                model, optimizer, \n",
    "                tf.reshape(tf.cast(tf.io.decode_raw(logits, tf.uint8), tf.float32), (-1, 224, 224, 3)), \n",
    "                tf.concat([label[..., tf.newaxis], oh_label, coord], axis=-1)\n",
    "            )\n",
    "        total_md_loss += mini_det_loss.numpy()\n",
    "        total_loss += model_loss.numpy()\n",
    "\n",
    "        step += 1\n",
    "        if step == train_size:\n",
    "            break\n",
    "    loss_history['train_loss'].append((total_md_loss / train_size, total_loss / train_size))\n",
    "\n",
    "    total_md_loss, total_loss, step = 0, 0, 0\n",
    "    for batch in valid_dataset:\n",
    "        logits, coord, label, oh_label = batch\n",
    "        \n",
    "        mini_det_loss, model_loss = validate(\n",
    "                model, \n",
    "                tf.reshape(tf.cast(tf.io.decode_raw(logits, tf.uint8), tf.float32), (-1, 224, 224, 3)), \n",
    "                tf.concat([label[..., tf.newaxis], oh_label, coord], axis=-1)\n",
    "            )\n",
    "        total_md_loss += mini_det_loss.numpy()\n",
    "        total_loss += model_loss.numpy()\n",
    "\n",
    "        step += 1\n",
    "        if step == valid_size:\n",
    "            break\n",
    "    loss_history['valid_loss'].append((total_md_loss / valid_size, total_loss / valid_size))\n",
    "    \n",
    "    # Save parameters after each epoch\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, f'ckpt_{epoch_idx}')\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    print(f'''epoch {epoch_idx+1:>2}: \\n\n",
    "          \\t train_loss: {loss_history[\"train_loss\"][-1][0]:.4f} {loss_history[\"train_loss\"][-1][1]:.4f},\n",
    "          \\t valid loss: {loss_history[\"valid_loss\"][-1][0]:.4f} {loss_history[\"valid_loss\"][-1][1]:.4f}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('/workspace/models/destr_20')\n",
    "#weights = model.get_weight()\n",
    "#model.set_weights(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
